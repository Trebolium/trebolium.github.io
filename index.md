---
layout: default
---

CV available [here](https://github.com/Trebolium/trebolium.github.io/blob/master/CV.pdf)

## PhD Candidate

I'm Brendan. I am a PhD candidate at the [Centre for Digital Music](http://c4dm.eecs.qmul.ac.uk/) (C4DM), in the Electrical Engineering and Computer Science department (EECS), at Queen Mary University of London (QMUL). My research is focused on singing attribute conversion with neural networks.

My research at C4DM has allowed me to work intimately in the field of Machine Learning and Audio. While working here I have covered topics such as:

- Languages: Python, Linux, git, php, xml
- Libraries: Pytorch, Numpy, Pandas, Scikit-learn
- Data Management: Collection, Mining, Preprocessing
- Algorithms: Search, Evolutionary, Neural Networks
- Neural Networks: CNNs, RNNs, Autoencoders, VAEs, GANs, Vocoders, Transformers, Diffusers
- Other Predictive Algorithms: Linear and Logistic Regression, SVMs, Decision Trees, Clustering
- Task Types: Discriminative and Generative Tasks, Supervised and Unsupervised Learning Techniques
- PhD Projects: Voice Identification, Style Classification, Attribute Disentablement, Attribute Conversion, Audio Synthesis
- Other Projects: Beat-tracking, Melodic Estimation, Audio Fingerprinting, Singing Voice Detection, Spoken Conversation Analysis
- Experiment Design: User Interface, Listening Studies, Evaluation Strategies, Statistical Analysis

While working at C4DM, I have had the pleasure of working as a teaching assistant for a number of undergraduate and postgraduate courses such as:

- Principles of Machine Learning
- Artificial Intelligence
- Python Programming
- Creating Interactive Objects
- Digital Audio
- Professional Research Practice

## Background

However, my background is immersed in music technology, performance, composition, and teaching.
See my CV for more information about this.

## Repositories to Visit

Wait till I've finished submiting my PhD in a few weeks! I plan to clean this section up. In the mean time, you are welcome to visit the repository on Perceptual Spaces of the Singing Voice [here](https://github.com/Trebolium/VoicePerception).

Repositories that you probably shouldn't yet look at. But here they are for you, at my own peril, alongside the relevant description:

- Title: Singing Technique Classification, [Repo link](https://github.com/Trebolium/VocalTechClass), [Paper Link](https://arxiv.org/abs/2111.08839)
- Title: Singing Technique Conversion, [Repo link](https://github.com/Trebolium/autoSTC), [Paper Link](https://arxiv.org/abs/2111.08839)
- Title: Singer Identity Conversion, [Repo link](https://github.com/Trebolium/autoSvc), [Paper Link](https://arxiv.org/abs/2302.13678)
- Title: Beat Tracker, [Repo Link](https://github.com/Trebolium/beat_tracker), Module on Music Informatics
- Title: Audio Fingerprinting, [Repo Link](https://github.com/Trebolium/shazam_Imitator), Module on Music Information Retrieval
- Title: Custom MIR Toolkit, [Repo Link](https://github.com/Trebolium/my_utils), 
<!-- ## Voice Work

Our most recent work focuses on generating a voice timbre encoder designed specifically for the singing voice. [Previous work](https://program.ismir2020.net/poster_1-08.html) exploring voice conversion in the singing domain have used encoder's trained on speech data to achieve the task of singing voice conversion. We have used a similar architecture proposed by [Wan et al. 2018](https://ieeexplore.ieee.org/abstract/document/8462665) and implemented by [CorentinJ](https://github.com/CorentinJ/Real-Time-Voice-Cloning) for this task, and trained it on a number of features and combinations of datasets. The implementation of this network is featured [here](https://github.com/Trebolium/singer_id_encoder).


**Commented on 2022.02.22 -> Please note that documentation and formatting for the public eye is underway. Links to _some_ of the repos below have been temporarily disabled**

Our published work on [Zero-shot Singing Voice Conversion](https://cmmr2021.github.io/proceedings/pdffiles/cmmr2021_26.pdf) describes a process for converting the perceived singing technique of a sung passage to a target technique, without affecting any other vocal attributes. The framework involves using the AutoVC framework (a repository for this has kindly supplied by the author [Kaizhi Qian/Auspicious3000](https://github.com/auspicious3000/autovc)) which is conditioned on the output embeddings of a pretrained singing technique classifier (the code of which is available [here](https://github.com/Trebolium/VocalTechClass)) The presentation can be found on the CMMR2021 Youtube channel [here](https://www.youtube.com/watch?v=3SpzDQKQ3O0&t=3283s).

Our more recent work explores the [WORLD vocoder](https:) for its voice-specific features and pitch-invariant properties. The WORLD vocoder is used to train a _singer-identity_ encoder, which captures the most important features of the vocal timbre and provides these as embeddings. This will be used to train an autoVC, the bottleneck embeddings of which we look forward to using as input to a VAE.

To ensure our models' latent space is reflecting something similar to that of human perception, we derived dissimilarity ratings by publishing a listening test, where users rated how different vocal sounds were from one another. The setup, analysis and conclusions are all documented in our paper, [An Exploratory Study on Perceptual Spaces of the Singing Voice](https://boblsturm.github.io/aimusic2020/papers/CSMC__MuMe_2020_paper_38.pdf) and illustrated at the Joint AI in Music Creativity conference [presentation](https://www.youtube.com/watch?v=DAZZ_ChbfSo). Results and analysis are presented [here](https://github.com/Trebolium/VoicePerception)!


## MIR Tasks

I also have experience in DSP, which has been made applicable in my studies on music information retrieval. An example of this can be found in the [Shazam imitator repo](https://github.com/Trebolium/shazam_Imitator), were recorded clips of a song in a noisey environment can be submitted as a query and compared with a song database. The algorithm returns the top three most likely matches for the given query. Results were evaluated using a subset of classical and pop songs from the GTZAN dataset. The algorithm is inspired from [Fundamentals of Music Processing](https://link.springer.com/book/10.1007/978-3-319-21945-5) (Muller, 2015).

I have also designed a basic beat-tracker that estimates tempo and follows the beat of music, using a combination of techniques from multiple researchers. Results were evaluated using the [Ballroom dataset](http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html). The repository for this can be found [here](https://github.com/Trebolium/beat_tracker).

Separate documentation providing further referencing and context for these applications for these repositories are available on request. -->
